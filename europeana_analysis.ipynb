{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b731508",
   "metadata": {},
   "source": [
    "# Creative FLIP collab session at MILL\n",
    "\n",
    "The following are example analysis using the Europeana API. These can serve as a starting point for testing out ideas or creating new analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a27b",
   "metadata": {},
   "source": [
    "## Setup Europeana client\n",
    "\n",
    "The following code sets up the Europeana API client. You need to run it at least onece before running the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import collections\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from europeana_client import (\n",
    "    EuropeanaClient,\n",
    "    AggregateField,\n",
    "    SearchField,\n",
    "    MediaType,\n",
    "    Profile,\n",
    "    Reusability,\n",
    ")\n",
    "from europeana_models import EuropeanaItem\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "api_key = os.getenv(\"EUROPEANA_API_KEY\")\n",
    "# api_key = None\n",
    "if not api_key:\n",
    "    # If no environment variable is set, you can enter your API key directly here\n",
    "    api_key = input(\"Enter your Europeana API key: \")\n",
    "\n",
    "print(f\"API key loaded: {'Yes' if api_key else 'No'}\")\n",
    "\n",
    "client = EuropeanaClient(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdb739",
   "metadata": {},
   "source": [
    "## Example 1: Gap in data - Place but no location\n",
    "\n",
    "Here we give an example of analysing the dataset with regards to gaps in the data. Items in the Europeana database have an atribute named `dcterms:spatial` defined as the following\n",
    "\n",
    "```\n",
    "Information about the spatial characteristics of the original analog or born digital object (e.g. a named place, location, coordinates, or administrative entity)\n",
    "```\n",
    "\n",
    "Items also have other atributes that also describe their spatial characteristics, e.g. `pl_wgs84_pos_lat` and `pl_wgs84_pos_long` describing latitude and longitude of the item for a more precise location. We want to see what portions of items have `dcterms:spatial` but are missing the other spatial atributes resulting in incomplete spatial data for the item.\n",
    "\n",
    "For this example we will look at items of `MediaType.IMAGE` by `Rembrandt`.\n",
    "\n",
    "First we cunstruct our query and execute the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae84241",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_1 = client.query().who(\"Rembrandt\").media_type(MediaType.IMAGE)\n",
    "result_ex_1 = list(client.search_all(query_ex_1, max_records=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3af65",
   "metadata": {},
   "source": [
    "Now we can do the comparison for each item in our results. Lets start by looking at if dcterms:spatial exist wether latidue and longitude exists also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_missing_lat_long = 0\n",
    "\n",
    "for item in result_ex_1:\n",
    "    if item.dcterms_spatial and (\n",
    "        not item.edm_place_latitude or not item.edm_place_longitude\n",
    "    ):\n",
    "        items_missing_lat_long += 1\n",
    "\n",
    "print(f\"Total items: {len(result_ex_1)}\")\n",
    "print(\n",
    "    f\"Items with dcterms:spatial but missing latitude/longitude: {items_missing_lat_long}\"\n",
    ")\n",
    "print(f\"Percentage: {items_missing_lat_long / len(result_ex_1) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d76a1e",
   "metadata": {},
   "source": [
    "Now we see that there exist some amount of items that have a spatial description but still don't have precice latitude or longitude.\n",
    "\n",
    "But what kind of information is actually encoded in `dcterms:spatial`? Maybe we have access to latitude and longitute there? Lets have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f14eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result_ex_1[\n",
    "    :50:5\n",
    "]:  # Print every 5th item of the first 50 items to limit output\n",
    "    print(item.dcterms_spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5941c",
   "metadata": {},
   "source": [
    "Now we see some interesting things. `dcterms:spatial` can include more than one entry and many of the entries are links to external resources. Some of these links describe large areas (e.g. Germany) and smaller areas (e.g. Hannover region). Sometimes there is a place name, f.x. `Suomi, Uusimaa, Helsinki, Kaartinkaupunki Helsinki, Pohjoisesplanadi 2`. Some entries link to more precise entities, like [http://data.europeana.eu/organization/4562](http://data.europeana.eu/organization/4562') which links to Statens Museum of Kunst in Copenhagen Denmark.\n",
    "\n",
    "What we can see here is that `dcterms:spatial` can include a wide range of different formats. This makes working with the data in this field problematic as it is not consistent internally, i.e. is not all of the same type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b9a5c",
   "metadata": {},
   "source": [
    "# Example 2: How much of items are from outside of the EU\n",
    "\n",
    "Here we want want to try to see what proportion of items have a spatial reference to outside the EU. First we need to compile a list of EU countries as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab674fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of EU countries for reference\n",
    "eu_countries = [\n",
    "    \"Austria\",\n",
    "    \"Belgium\",\n",
    "    \"Bulgaria\",\n",
    "    \"Croatia\",\n",
    "    \"Cyprus\",\n",
    "    \"Czech Republic\",\n",
    "    \"Denmark\",\n",
    "    \"Estonia\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Greece\",\n",
    "    \"Hungary\",\n",
    "    \"Ireland\",\n",
    "    \"Italy\",\n",
    "    \"Latvia\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Malta\",\n",
    "    \"Netherlands\",\n",
    "    \"Poland\",\n",
    "    \"Portugal\",\n",
    "    \"Romania\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Sweden\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c725d",
   "metadata": {},
   "source": [
    "Now we can formulate the query and execute the search. Lets look at MediaType.IMAGES from 1650 to 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184048d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_2 = client.query().when(\"1650-1700\").media_type(MediaType.IMAGE)\n",
    "result_ex_2 = list(client.search_all(query_ex_2, max_records=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd1192",
   "metadata": {},
   "source": [
    "Now for the analysis. We will compare the `COUNTRY` field with the data in `dc_terms_spatial`. \n",
    "`dc_terms_spatial` can contain multiple entries of various types, e.g. place names or links. Sometimes the links are Europeana Place entities and sometimes they are links to external resources.\n",
    "\n",
    "We are going to do two types of checks here. If there `dc_terms_spatial` contains a link to a Europeana Place entity we will retrieve those entities and check if those correspond to a country. If the country is not on the `eu_countries` list we will increment our `non_eu_country_counter`. If there is no Europeana Place entity link we will do a simple string comparison to see if any of the strings corresponds to a country in the `eu_countries` list. If none does we will also increment the counter.\n",
    "\n",
    "The `dc_terms_spatial` atribute can also contain links to other external sources (e.g. geonames.org links) or strings describing geographic entities that are not countries, e.g. cities or regions, but we leave these out of the scope of the analysis to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_eu_country_counter = 0\n",
    "no_dc_terms_spatial_counter = 0\n",
    "total_items = len(result_ex_2)\n",
    "\n",
    "\n",
    "for item in result_ex_2:\n",
    "    if (\n",
    "        spatial := item.dcterms_spatial\n",
    "    ):  # First we check if there is any data in dcterms:spatial\n",
    "\n",
    "        # get_palce_entities_from_atribute returns a tuple (List[PlaceEntity], List[str])\n",
    "        # where the first list contains the PlaceEntities found, the second list contains\n",
    "        # any errors that occurred during processing and the third list contains\n",
    "        # the strings that were not recognized as PlaceEntities.\n",
    "        place_entities = client.get_place_entities_from_atribute(spatial)\n",
    "        if place_entities[\n",
    "            0\n",
    "        ]:  # If there are place entities we check if any of them correspond to an EU country\n",
    "            if not any(pe.get_name(\"en\") in eu_countries for pe in place_entities[0]):\n",
    "                non_eu_country_counter += 1\n",
    "        else:  # If there are no place entities we do a simple string comparison\n",
    "            if not any(s in eu_countries for s in place_entities[2]):\n",
    "                non_eu_country_counter += 1\n",
    "    else:  # If there is no data in dcterms:spatial we increment the counter\n",
    "        no_dc_terms_spatial_counter += 1\n",
    "\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Items with dcterms:spatial: {total_items - no_dc_terms_spatial_counter}\")\n",
    "print(\n",
    "    f\"Percentage of items with dcterms:spatial: {((total_items - no_dc_terms_spatial_counter) / total_items) * 100:.2f}%\"\n",
    ")\n",
    "print(f\"Items with dcterms:spatial but not from EU country: {non_eu_country_counter}\")\n",
    "print(\n",
    "    f\"Percentage of items with dcterms:spatial but not from EU country: {(non_eu_country_counter / (total_items - no_dc_terms_spatial_counter)) * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f762",
   "metadata": {},
   "source": [
    "## Example 5: timestamp created vs updated, more complex analysis\n",
    "\n",
    "Same time/date created and updated may indicate that a collection data is outdated or underresearched. This can make it easier to identify topics that need revision or special attention.\n",
    "\n",
    "In this example we compare items kept at German institutions that have a Palestinian (dataset 1) and Israeli (dataset 2) origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for analysis\n",
    "\n",
    "\n",
    "def extract_timestamps(results):\n",
    "    \"\"\"Extract and parse timestamps from search results.\"\"\"\n",
    "    created_timestamps = []\n",
    "    updated_timestamps = []\n",
    "    missing_created = 0\n",
    "    missing_updated = 0\n",
    "    missing_both = 0\n",
    "\n",
    "    print(f\"Analyzing {len(results)} items for timestamp data...\")\n",
    "\n",
    "    for item in results:\n",
    "        created = item.timestamp_created\n",
    "        updated = item.timestamp_update\n",
    "\n",
    "        # Track missing data\n",
    "        if created is None and updated is None:\n",
    "            missing_both += 1\n",
    "        elif created is None:\n",
    "            missing_created += 1\n",
    "        elif updated is None:\n",
    "            missing_updated += 1\n",
    "\n",
    "        # Only add to plot data if both timestamps are available\n",
    "        if created is not None and updated is not None:\n",
    "            # Convert ISO timestamp strings to datetime\n",
    "            try:\n",
    "                created_dt = datetime.fromisoformat(created.replace(\"Z\", \"+00:00\"))\n",
    "                updated_dt = datetime.fromisoformat(updated.replace(\"Z\", \"+00:00\"))\n",
    "                created_timestamps.append(created_dt)\n",
    "                updated_timestamps.append(updated_dt)\n",
    "            except (ValueError, AttributeError) as e:\n",
    "                print(f\"Error parsing timestamps: {created}, {updated} - {e}\")\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\nTimestamp Statistics:\")\n",
    "    print(f\"Total items analyzed: {len(results)}\")\n",
    "    print(f\"Items with both timestamps: {len(created_timestamps)}\")\n",
    "    print(f\"Items missing creation timestamp only: {missing_created}\")\n",
    "    print(f\"Items missing update timestamp only: {missing_updated}\")\n",
    "    print(f\"Items missing both timestamps: {missing_both}\")\n",
    "    print(f\"Data availability: {len(created_timestamps)/len(results)*100:.1f}%\")\n",
    "\n",
    "    return created_timestamps, updated_timestamps\n",
    "\n",
    "\n",
    "def create_timestamp_plot(created_timestamps, updated_timestamps, location):\n",
    "    \"\"\"Create scatter plot for timestamp analysis.\"\"\"\n",
    "    if not created_timestamps:\n",
    "        print(f\"No data available for plotting {location}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(created_timestamps, updated_timestamps, alpha=0.6, s=20)\n",
    "\n",
    "    # Add diagonal line to show where created = updated\n",
    "    min_date = min(min(created_timestamps), min(updated_timestamps))\n",
    "    max_date = max(max(created_timestamps), max(updated_timestamps))\n",
    "    plt.plot(\n",
    "        [min_date, max_date],\n",
    "        [min_date, max_date],\n",
    "        \"r--\",\n",
    "        alpha=0.7,\n",
    "        label=\"Created = Updated line\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Timestamp Created\")\n",
    "    plt.ylabel(\"Timestamp Updated\")\n",
    "    plt.title(\n",
    "        f\"Creation vs Update Timestamps for {location} Items\\n({len(created_timestamps)} items with complete timestamp data)\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_temporal_analysis(created_timestamps, updated_timestamps, location):\n",
    "    \"\"\"Perform comprehensive temporal analysis and print insights.\"\"\"\n",
    "    if not created_timestamps:\n",
    "        print(f\"No timestamp data available for {location} analysis\")\n",
    "        return\n",
    "\n",
    "    # Analyze items where created == updated (potential indicators of outdated/static collections)\n",
    "    same_timestamp_count = sum(\n",
    "        1 for c, u in zip(created_timestamps, updated_timestamps) if c == u\n",
    "    )\n",
    "    print(f\"\\n=== {location.upper()} ANALYSIS ===\")\n",
    "    print(\n",
    "        f\"Items with identical creation and update timestamps: {same_timestamp_count}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Percentage of items never updated: {same_timestamp_count/len(created_timestamps)*100:.1f}%\"\n",
    "    )\n",
    "\n",
    "    # Show time range of the data\n",
    "    earliest_created = min(created_timestamps)\n",
    "    latest_created = max(created_timestamps)\n",
    "    earliest_updated = min(updated_timestamps)\n",
    "    latest_updated = max(updated_timestamps)\n",
    "\n",
    "    print(f\"\\nTime Range Analysis:\")\n",
    "    print(f\"Earliest creation date: {earliest_created}\")\n",
    "    print(f\"Latest creation date: {latest_created}\")\n",
    "    print(f\"Earliest update date: {earliest_updated}\")\n",
    "    print(f\"Latest update date: {latest_updated}\")\n",
    "\n",
    "    # Calculate average update time (time between creation and update)\n",
    "    current_time = datetime.now(\n",
    "        created_timestamps[0].tzinfo\n",
    "    )  # Use same timezone as data\n",
    "    update_intervals = []\n",
    "    item_ages = []\n",
    "\n",
    "    for created, updated in zip(created_timestamps, updated_timestamps):\n",
    "        # Calculate time between creation and update\n",
    "        if created != updated:  # Only for items that were actually updated\n",
    "            update_interval = (updated - created).total_seconds() / (\n",
    "                24 * 3600\n",
    "            )  # Convert to days\n",
    "            update_intervals.append(update_interval)\n",
    "\n",
    "        # Calculate age of item (creation to now)\n",
    "        age = (current_time - created).total_seconds() / (24 * 3600)  # Convert to days\n",
    "        item_ages.append(age)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"TEMPORAL ANALYSIS - {location.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Average update time analysis\n",
    "    if update_intervals:\n",
    "        avg_update_time_days = np.mean(update_intervals)\n",
    "        median_update_time_days = np.median(update_intervals)\n",
    "        min_update_time_days = np.min(update_intervals)\n",
    "        max_update_time_days = np.max(update_intervals)\n",
    "        std_update_time_days = np.std(update_intervals)\n",
    "\n",
    "        print(f\"\\nUpdate Time Analysis (for {len(update_intervals)} updated items):\")\n",
    "        print(\n",
    "            f\"  Average time to update: {avg_update_time_days:.1f} days ({avg_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Median time to update:  {median_update_time_days:.1f} days ({median_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Fastest update:         {min_update_time_days:.1f} days\")\n",
    "        print(\n",
    "            f\"  Slowest update:         {max_update_time_days:.1f} days ({max_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Standard deviation:     {std_update_time_days:.1f} days\")\n",
    "    else:\n",
    "        print(f\"\\nUpdate Time Analysis:\")\n",
    "        print(f\"  No items were updated after creation (all have identical timestamps)\")\n",
    "\n",
    "    # Average age analysis\n",
    "    if item_ages:\n",
    "        avg_age_days = np.mean(item_ages)\n",
    "        median_age_days = np.median(item_ages)\n",
    "        min_age_days = np.min(item_ages)\n",
    "        max_age_days = np.max(item_ages)\n",
    "        std_age_days = np.std(item_ages)\n",
    "\n",
    "        print(f\"\\nItem Age Analysis (creation to present):\")\n",
    "        print(\n",
    "            f\"  Average age of items:   {avg_age_days:.1f} days ({avg_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Median age of items:    {median_age_days:.1f} days ({median_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Newest item:            {min_age_days:.1f} days old\")\n",
    "        print(\n",
    "            f\"  Oldest item:            {max_age_days:.1f} days old ({max_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Standard deviation:     {std_age_days:.1f} days ({std_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "\n",
    "    # Summary insights\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"INSIGHTS - {location.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    never_updated_pct = same_timestamp_count / len(created_timestamps) * 100\n",
    "    print(f\"• {never_updated_pct:.1f}% of items have never been updated since creation\")\n",
    "\n",
    "    if update_intervals and len(update_intervals) > 0:\n",
    "        updated_pct = len(update_intervals) / len(created_timestamps) * 100\n",
    "        print(f\"• {updated_pct:.1f}% of items have been updated at least once\")\n",
    "        print(\n",
    "            f\"• Items that get updated wait an average of {avg_update_time_days/365.25:.1f} years before their first update\"\n",
    "        )\n",
    "\n",
    "    if item_ages:\n",
    "        print(\n",
    "            f\"• The average item in this collection is {avg_age_days/365.25:.1f} years old\"\n",
    "        )\n",
    "\n",
    "        # Age distribution insight\n",
    "        recent_items = sum(1 for age in item_ages if age <= 365)\n",
    "        old_items = sum(1 for age in item_ages if age >= 365 * 5)\n",
    "        recent_pct = recent_items / len(item_ages) * 100\n",
    "        old_pct = old_items / len(item_ages) * 100\n",
    "\n",
    "        print(f\"• {recent_pct:.1f}% of items were created within the last year\")\n",
    "        print(f\"• {old_pct:.1f}% of items are more than 5 years old\")\n",
    "\n",
    "\n",
    "print(\"Analysis functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310cba1",
   "metadata": {},
   "source": [
    "## Fetch the data\n",
    "\n",
    "This can take some minutes if you are fetching more than 10000 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8aa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = EuropeanaClient(api_key)\n",
    "\n",
    "# Fetch first dataset\n",
    "print(\"Fetching first dataset...\")\n",
    "query_1 = client.query().country(\"Germany\").where(\"Israel\")\n",
    "results_1 = list(client.search_all(query_1, max_records=10000))\n",
    "\n",
    "# Fetch second dataset\n",
    "print(\"Fetching second dataset...\")\n",
    "query_2 = client.query().country(\"Germany\").where(\"Palästina\")\n",
    "results_2 = list(client.search_all(query_2, max_records=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a8b8e",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vo8bkbbqvsr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for Dataset 1\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 1 ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "created_1, updated_1 = extract_timestamps(results_1)\n",
    "create_timestamp_plot(created_1, updated_1, \"Dataset 1\")\n",
    "perform_temporal_analysis(created_1, updated_1, \"Dataset 1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET 2 ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run analysis for Dataset 2\n",
    "created_2, updated_2 = extract_timestamps(results_2)\n",
    "create_timestamp_plot(created_2, updated_2, \"Dataset 2\")\n",
    "perform_temporal_analysis(created_2, updated_2, \"Dataset 2\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARATIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if created_1 and created_2:\n",
    "    never_updated_1 = sum(1 for c, u in zip(created_1, updated_1) if c == u)\n",
    "    never_updated_2 = sum(1 for c, u in zip(created_2, updated_2) if c == u)\n",
    "\n",
    "    never_pct_1 = never_updated_1 / len(created_1) * 100\n",
    "    never_pct_2 = never_updated_2 / len(created_2) * 100\n",
    "\n",
    "    print(f\"Dataset 1: {len(results_1)} items found, {len(created_1)} with timestamps\")\n",
    "    print(f\"Dataset 2: {len(results_2)} items found, {len(created_2)} with timestamps\")\n",
    "    print(f\"\\nNever updated comparison:\")\n",
    "    print(f\"• Dataset 1: {never_pct_1:.1f}% of items never updated\")\n",
    "    print(f\"• Dataset 2: {never_pct_2:.1f}% of items never updated\")\n",
    "\n",
    "    if never_pct_1 > never_pct_2:\n",
    "        diff = never_pct_1 - never_pct_2\n",
    "        print(\n",
    "            f\"• Dataset 1 has {diff:.1f} percentage points more items that were never updated\"\n",
    "        )\n",
    "    elif never_pct_2 > never_pct_1:\n",
    "        diff = never_pct_2 - never_pct_1\n",
    "        print(\n",
    "            f\"• Dataset 2 has {diff:.1f} percentage points more items that were never updated\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"• Both datasets have similar rates of items never updated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
