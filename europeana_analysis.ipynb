{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b731508",
   "metadata": {},
   "source": [
    "# Creative FLIP collab session at MILL\n",
    "\n",
    "The following are example analysis using the Europeana API. These can serve as a starting point for testing out ideas or creating new analysis.\n",
    "\n",
    "## Memory Issues\n",
    "\n",
    "When running this notebook in mybinder.org we can run into memory issues because of all the data we are downloading and processing and the fact that that mybinder.org only provieds 2 GBs of RAM per instance. \n",
    "\n",
    "We have added code to delete query results and run the garbage collector after each example. Feel free to skip running the cleanu code in case you want to reatry some examples, just be mindfull of your memory usage, else you might crash your notebook. Memory usage is visible at the bottom of the notebook window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8991a27b",
   "metadata": {},
   "source": [
    "## Setup Europeana client\n",
    "\n",
    "The following code sets up the Europeana API client. You need to run it at least onece before running the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aa5b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import collections\n",
    "import gc\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Any, Dict, List, Optional, Tuple, TypedDict, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from europeana import (\n",
    "    AggregateField,\n",
    "    EuropeanaClient,\n",
    "    EuropeanaItem,\n",
    "    MediaType,\n",
    "    Profile,\n",
    "    Reusability,\n",
    "    SearchField,\n",
    ")\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "api_key = os.getenv(\"EUROPEANA_API_KEY\")\n",
    "# api_key = None\n",
    "if not api_key:\n",
    "    # If no environment variable is set, you can enter your API key directly here\n",
    "    api_key = input(\"Enter your Europeana API key: \")\n",
    "\n",
    "print(f\"API key loaded: {'Yes' if api_key else 'No'}\")\n",
    "\n",
    "client = EuropeanaClient(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fdb739",
   "metadata": {},
   "source": [
    "## Example 1: Gap in data - Place but no location\n",
    "\n",
    "Here we give an example of analysing the dataset with regards to gaps in the data. Items in the Europeana database have an atribute named `dcterms:spatial` defined as the following\n",
    "\n",
    "```\n",
    "Information about the spatial characteristics of the original analog or born digital object (e.g. a named place, location, coordinates, or administrative entity)\n",
    "```\n",
    "\n",
    "Items also have other atributes that also describe their spatial characteristics, e.g. `pl_wgs84_pos_lat` and `pl_wgs84_pos_long` describing latitude and longitude of the item for a more precise location. We want to see what portions of items have `dcterms:spatial` but are missing the other spatial atributes resulting in incomplete spatial data for the item.\n",
    "\n",
    "For this example we will look at items of `MediaType.IMAGE` by `Rembrandt`.\n",
    "\n",
    "First we cunstruct our query and execute the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae84241",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_1 = client.query().who(\"Rembrandt\").media_type(MediaType.IMAGE)\n",
    "result_ex_1 = list(client.search_all(query_ex_1, max_records=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3af65",
   "metadata": {},
   "source": [
    "Now we can do the comparison for each item in our results. Lets start by looking at if dcterms:spatial exist wether latidue and longitude exists also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c77bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_missing_lat_long = 0\n",
    "\n",
    "for item in result_ex_1:\n",
    "    if item.dcterms_spatial and (\n",
    "        not item.edm_place_latitude or not item.edm_place_longitude\n",
    "    ):\n",
    "        items_missing_lat_long += 1\n",
    "\n",
    "print(f\"Total items: {len(result_ex_1)}\")\n",
    "print(\n",
    "    f\"Items with dcterms:spatial but missing latitude/longitude: {items_missing_lat_long}\"\n",
    ")\n",
    "print(f\"Percentage: {items_missing_lat_long / len(result_ex_1) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d76a1e",
   "metadata": {},
   "source": [
    "Now we see that there exist some amount of items that have a spatial description but still don't have precice latitude or longitude.\n",
    "\n",
    "But what kind of information is actually encoded in `dcterms:spatial`? Maybe we have access to latitude and longitute there? Lets have a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f14eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in result_ex_1[\n",
    "    :50:5\n",
    "]:  # Print every 5th item of the first 50 items to limit output\n",
    "    print(item.dcterms_spatial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc5941c",
   "metadata": {},
   "source": [
    "Now we see some interesting things. `dcterms:spatial` can include more than one entry and many of the entries are links to external resources. Some of these links describe large areas (e.g. Germany) and smaller areas (e.g. Hannover region). Sometimes there is a place name, f.x. `Suomi, Uusimaa, Helsinki, Kaartinkaupunki Helsinki, Pohjoisesplanadi 2`. Some entries link to more precise entities, like [http://data.europeana.eu/organization/4562](http://data.europeana.eu/organization/4562') which links to Statens Museum of Kunst in Copenhagen Denmark.\n",
    "\n",
    "What we can see here is that `dcterms:spatial` can include a wide range of different formats. This makes working with the data in this field problematic as it is not consistent internally, i.e. is not all of the same type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81201ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del result_ex_1\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b9a5c",
   "metadata": {},
   "source": [
    "# Example 2: How much of items are from outside of the EU\n",
    "\n",
    "Here we want want to try to see what proportion of items have a spatial reference to outside the EU. First we need to compile a list of EU countries as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab674fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of EU countries for reference. Some countries seem to use official full names, so we include those as well.\n",
    "eu_countries = [\n",
    "    \"Austria\",\n",
    "    \"Belgium\",\n",
    "    \"Bulgaria\",\n",
    "    \"Croatia\",\n",
    "    \"Cyprus\",\n",
    "    \"Czech Republic\",\n",
    "    \"Denmark\",\n",
    "    \"Estonia\",\n",
    "    \"Finland\",\n",
    "    \"France\",\n",
    "    \"Germany\",\n",
    "    \"Greece\",\n",
    "    \"Hungary\",\n",
    "    \"Ireland\",\n",
    "    \"Italy\",\n",
    "    \"Latvia\",\n",
    "    \"Lithuania\",\n",
    "    \"Luxembourg\",\n",
    "    \"Malta\",\n",
    "    \"Netherlands\",\n",
    "    \"Poland\",\n",
    "    \"Portugal\",\n",
    "    \"Romania\",\n",
    "    \"Slovakia\",\n",
    "    \"Slovenia\",\n",
    "    \"Spain\",\n",
    "    \"Sweden\",\n",
    "    \"Republic of Austria\",\n",
    "    \"Kingdom of Belgium\",\n",
    "    \"Republic of Bulgaria\",\n",
    "    \"Republic of Croatia\",\n",
    "    \"Republic of Cyprus\",\n",
    "    \"Czech Republic\",\n",
    "    \"Kingdom of Denmark\",\n",
    "    \"Republic of Estonia\",\n",
    "    \"Republic of Finland\",\n",
    "    \"French Republic\",\n",
    "    \"Federal Republic of Germany\",\n",
    "    \"Hellenic Republic\",\n",
    "    \"Republic of Hungary\",\n",
    "    \"Ireland\",\n",
    "    \"Italian Republic\",\n",
    "    \"Republic of Latvia\",\n",
    "    \"Republic of Lithuania\",\n",
    "    \"Grand Duchy of Luxembourg\",\n",
    "    \"Republic of Malta\",\n",
    "    \"Kingdom of the Netherlands\",\n",
    "    \"Republic of Poland\",\n",
    "    \"Portuguese Republic\",\n",
    "    \"Romania\",\n",
    "    \"Slovak Republic\",\n",
    "    \"Republic of Slovenia\",\n",
    "    \"Kingdom of Spain\",\n",
    "    \"Kingdom of Sweden\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c725d",
   "metadata": {},
   "source": [
    "Now we can formulate the query and execute the search. Lets look at MediaType.IMAGES from 1650 to 1700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184048d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_2 = client.query().when(\"1650-1700\").media_type(MediaType.IMAGE)\n",
    "result_ex_2 = list(client.search_all(query_ex_2, max_records=100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd1192",
   "metadata": {},
   "source": [
    "Now for the analysis.\n",
    "`dc_terms_spatial` can contain multiple entries of various types, e.g. place names or links. Sometimes the links are Europeana Place entities and sometimes they are links to external resources.\n",
    "\n",
    "We are going to do two types of checks here. If there `dc_terms_spatial` contains a link to a Europeana Place entity we will retrieve those entities and check if those correspond to a country. If the country is not on the `eu_countries` list we will increment our `non_eu_country_counter`. If there is no Europeana Place entity link we will do a simple string comparison to see if any of the strings corresponds to a country in the `eu_countries` list. If none does we will also increment the counter.\n",
    "\n",
    "The `dc_terms_spatial` atribute can also contain links to other external sources (e.g. geonames.org links) or strings describing geographic entities that are not countries, e.g. cities or regions, but we leave these out of the scope of the analysis to keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_country_counter = 0\n",
    "no_dc_terms_spatial_counter = 0\n",
    "total_items = len(result_ex_2)\n",
    "\n",
    "\n",
    "for item in result_ex_2:\n",
    "    if (\n",
    "        spatial := item.dcterms_spatial\n",
    "    ):  # First we check if there is any data in dcterms:spatial\n",
    "\n",
    "        # get_palce_entities_from_atribute returns a tuple (List[PlaceEntity], List[str])\n",
    "        # where the first list contains the PlaceEntities found, the second list contains\n",
    "        # any errors that occurred during processing and the third list contains\n",
    "        # the strings that were not recognized as PlaceEntities.\n",
    "        place_entities = client.get_place_entities_from_atribute(spatial)\n",
    "        if place_entities[\n",
    "            0\n",
    "        ]:  # If there are place entities we check if any of them correspond to an EU country\n",
    "            if any(pe.get_name(\"en\") in eu_countries for pe in place_entities[0]):\n",
    "                eu_country_counter += 1\n",
    "        else:  # If there are no place entities we do a simple string comparison to the EU country list\n",
    "            if not any(s in eu_countries for s in place_entities[2]):\n",
    "                eu_country_counter += 1\n",
    "    else:  # If there is no data in dcterms:spatial we increment the counter\n",
    "        no_dc_terms_spatial_counter += 1\n",
    "\n",
    "print(f\"Total items: {total_items}\")\n",
    "print(f\"Items with dcterms:spatial: {total_items - no_dc_terms_spatial_counter}\")\n",
    "print(\n",
    "    f\"Percentage of items with dcterms:spatial: {((total_items - no_dc_terms_spatial_counter) / total_items) * 100:.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"Items with dcterms:spatial but no EU country reference: {total_items - eu_country_counter}\"\n",
    ")\n",
    "print(\n",
    "    f\"Percentage of items with dcterms:spatial but no EU country reference: {((total_items - no_dc_terms_spatial_counter - eu_country_counter) / (total_items - no_dc_terms_spatial_counter)) * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4db66",
   "metadata": {},
   "source": [
    "There is some limitations to this result. If one looks at the data we see that occationally the `dc_terms_spatial` only contains a region or a city, and not the Country that those regions or cities are located in. This will lead to false positive since we are not checking the location of each city or region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce57324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del result_ex_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd2de07",
   "metadata": {},
   "source": [
    "# Example 3: Redundancy\n",
    "\n",
    "Here we want to check how many fields are redundant, i.e. contain the same data.\n",
    "First we get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5556e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_3 = client.query().where(\"France\")\n",
    "result_ex_3 = list(client.search_all(query_ex_3, max_records=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb39dfa",
   "metadata": {},
   "source": [
    "The analysis here is a bit simple, we just check if strings in a field exists in any other.\n",
    "If a string exists in another field we mark it as a redundancy. This is not a very thorough analysis but might insights into what fields would warrant further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f15ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all field names from the first item (excluding private fields and methods).\n",
    "# This analysis might take some time, aproximately 1,5 min for every 10k items.\n",
    "if result_ex_3:\n",
    "    sample_item = result_ex_3[0]\n",
    "    all_fields = [\n",
    "        attr\n",
    "        for attr in dir(sample_item)\n",
    "        if not attr.startswith(\"_\") and not callable(getattr(sample_item, attr))\n",
    "    ]\n",
    "\n",
    "    # Remove some fields that don't make sense to compare. Maybe expand this list based on findings.\n",
    "    exclude_fields = [\n",
    "        \"aggregations\",\n",
    "        \"web_resources\",\n",
    "        \"index\",\n",
    "        \"score\",\n",
    "        \"completeness\",\n",
    "        \"europeana_completeness\",\n",
    "        \"timestamp\",\n",
    "        \"timestamp_created_epoch\",\n",
    "        \"timestamp_update_epoch\",\n",
    "        \"ugc\",\n",
    "        \"preview_no_distribute\",\n",
    "    ]\n",
    "\n",
    "    comparable_fields = [f for f in all_fields if f not in exclude_fields]\n",
    "\n",
    "    print(f\"Comparing {len(comparable_fields)} fields against each other...\")\n",
    "\n",
    "    redundant_pairs = []\n",
    "\n",
    "    # Compare every field with every other field\n",
    "    for i, field1 in enumerate(comparable_fields):\n",
    "        for field2 in comparable_fields[i + 1 :]:  # Only compare each pair once\n",
    "\n",
    "            overlap_count = 0\n",
    "            total_with_both = 0\n",
    "\n",
    "            for item in result_ex_3:\n",
    "                val1 = getattr(item, field1, None)\n",
    "                val2 = getattr(item, field2, None)\n",
    "\n",
    "                # Skip if either field is empty\n",
    "                if not val1 or not val2:\n",
    "                    continue\n",
    "\n",
    "                total_with_both += 1\n",
    "\n",
    "                # Normalize values for comparison since some fields are lists while others are simple strings.\n",
    "                def normalize(val):\n",
    "                    if isinstance(val, list):\n",
    "                        return set(str(v).lower().strip() for v in val if v)\n",
    "                    else:\n",
    "                        return {str(val).lower().strip()}\n",
    "\n",
    "                set1 = normalize(val1)\n",
    "                set2 = normalize(val2)\n",
    "\n",
    "                # Check for any overlap\n",
    "                if set1.intersection(set2):\n",
    "                    overlap_count += 1\n",
    "\n",
    "            # Calculate redundancy percentage\n",
    "            if total_with_both > 0:\n",
    "                redundancy_pct = (overlap_count / total_with_both) * 100\n",
    "                if redundancy_pct > 5:  # Only show significant redundancies\n",
    "                    redundant_pairs.append(\n",
    "                        {\n",
    "                            \"field1\": field1,\n",
    "                            \"field2\": field2,\n",
    "                            \"redundancy_pct\": redundancy_pct,\n",
    "                            \"overlap_count\": overlap_count,\n",
    "                            \"total_with_both\": total_with_both,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "    # Sort by redundancy percentage\n",
    "    redundant_pairs.sort(key=lambda x: x[\"redundancy_pct\"], reverse=True)\n",
    "\n",
    "    print(\n",
    "        f\"\\nFound {len(redundant_pairs)} field pairs with significant redundancy (>5%):\"\n",
    "    )\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for i, pair in enumerate(redundant_pairs[:10], 1):  # Show top 10\n",
    "        print(f\"{i:2d}. {pair['field1']} ↔ {pair['field2']}\")\n",
    "        print(\n",
    "            f\"    {pair['redundancy_pct']:.1f}% redundancy ({pair['overlap_count']}/{pair['total_with_both']} items)\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del result_ex_3\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fvnqqnfhsou",
   "metadata": {},
   "source": [
    "### Analysis Results\n",
    "\n",
    "We can immediately see some interesting things.\n",
    "\n",
    "edm_dataset_name and europeana_collection_name contain the same data 100 % of the time. If we look at the Search API documentation we see that europeana_collection_name is being replaced by edm_dataset_name so this is expected.\n",
    "\n",
    "The redundnacy in the spatial terms is more interesting and might indicate that there is some overlap there worth investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9vqkazi7yo6",
   "metadata": {},
   "source": [
    "# Example 4: Permissive Copyright but No Accessible Media\n",
    "\n",
    "One interesting topic to investigate is if all items with permissable copyright actually provide access to the media.\n",
    " \n",
    "This example analyzes items that have permissive copyright licenses but lack accessible media URLs. This gap represents missed opportunities for public access to cultural heritage content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wu55cg9aip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "class MediaInfo(TypedDict):\n",
    "    has_media: bool\n",
    "    media_types: List[str]\n",
    "    urls: List[str]\n",
    "\n",
    "\n",
    "class GapExample(TypedDict):\n",
    "    id: str | None\n",
    "    title: str | None\n",
    "    provider: str | None\n",
    "    license: str | None\n",
    "    link: str | None\n",
    "\n",
    "\n",
    "class MediaAvailabilityBreakdown(TypedDict):\n",
    "    edm_is_shown_by: int\n",
    "    edm_is_shown_at: int\n",
    "    edm_preview: int\n",
    "    web_resource: int\n",
    "    no_media: int\n",
    "\n",
    "\n",
    "class AnalysisResults(TypedDict):\n",
    "    total_items: int\n",
    "    permissive_copyright: int\n",
    "    has_accessible_media: int\n",
    "    permissive_with_media: int\n",
    "    permissive_without_media: int\n",
    "    gap_examples: List[GapExample]\n",
    "    permissive_licenses_found: Dict[str, int]\n",
    "    media_availability_breakdown: MediaAvailabilityBreakdown\n",
    "\n",
    "\n",
    "def is_permissive_copyright(item: EuropeanaItem) -> Tuple[bool, Optional[str]]:\n",
    "    \"\"\"Check if an item has permissive copyright allowing free use.\"\"\"\n",
    "    rights_uris = item.get_rights_uris()\n",
    "\n",
    "    # Patterns that indicate permissive/open licensing\n",
    "    permissive_patterns = [\n",
    "        \"creativecommons.org/publicdomain/zero\",  # CC0 - public domain dedication\n",
    "        \"creativecommons.org/licenses/by/\",  # CC-BY - attribution only\n",
    "        \"creativecommons.org/publicdomain/mark\",  # Public domain mark\n",
    "        \"rightsstatements.org/vocab/NoC\",  # No Copyright\n",
    "        \"rightsstatements.org/vocab/NKC\",  # No Known Copyright\n",
    "        \"rightsstatements.org/vocab/PDM\",  # Public Domain Mark\n",
    "        \"publicdomain\",  # General public domain\n",
    "        \"europeana.eu/rights/pd\",  # Europeana public domain\n",
    "    ]\n",
    "\n",
    "    # Check if any rights URI contains permissive patterns\n",
    "    for uri in rights_uris:\n",
    "        uri_lower = uri.lower()\n",
    "        for pattern in permissive_patterns:\n",
    "            if pattern.lower() in uri_lower:\n",
    "                return True, uri\n",
    "\n",
    "    return False, None\n",
    "\n",
    "\n",
    "def has_accessible_media(item: EuropeanaItem) -> MediaInfo:\n",
    "    \"\"\"Check if an item has accessible digital media URLs.\"\"\"\n",
    "    media_info: MediaInfo = {\"has_media\": False, \"media_types\": [], \"urls\": []}\n",
    "\n",
    "    # Check direct media URLs\n",
    "    if item.edm_is_shown_by:\n",
    "        media_info[\"has_media\"] = True\n",
    "        media_info[\"media_types\"].append(\"edm_is_shown_by\")\n",
    "        urls = (\n",
    "            item.edm_is_shown_by\n",
    "            if isinstance(item.edm_is_shown_by, list)\n",
    "            else [item.edm_is_shown_by]\n",
    "        )\n",
    "        media_info[\"urls\"].extend(urls)\n",
    "\n",
    "    if item.edm_is_shown_at:\n",
    "        media_info[\"has_media\"] = True\n",
    "        media_info[\"media_types\"].append(\"edm_is_shown_at\")\n",
    "        urls = (\n",
    "            item.edm_is_shown_at\n",
    "            if isinstance(item.edm_is_shown_at, list)\n",
    "            else [item.edm_is_shown_at]\n",
    "        )\n",
    "        media_info[\"urls\"].extend(urls)\n",
    "\n",
    "    if item.edm_preview:\n",
    "        media_info[\"has_media\"] = True\n",
    "        media_info[\"media_types\"].append(\"edm_preview\")\n",
    "        urls = (\n",
    "            item.edm_preview\n",
    "            if isinstance(item.edm_preview, list)\n",
    "            else [item.edm_preview]\n",
    "        )\n",
    "        media_info[\"urls\"].extend(urls)\n",
    "\n",
    "    # Check web resources\n",
    "    if item.web_resources and len(item.web_resources) > 0:\n",
    "        for resource in item.web_resources:\n",
    "            if resource.about:  # Has a URL\n",
    "                media_info[\"has_media\"] = True\n",
    "                media_info[\"media_types\"].append(\"web_resource\")\n",
    "                media_info[\"urls\"].append(resource.about)\n",
    "\n",
    "    # Remove duplicates from media_types\n",
    "    media_info[\"media_types\"] = list(set(media_info[\"media_types\"]))\n",
    "    media_info[\"urls\"] = list(set(media_info[\"urls\"]))\n",
    "\n",
    "    return media_info\n",
    "\n",
    "\n",
    "def analyze_permissive_no_media_gap(results: List[EuropeanaItem]) -> AnalysisResults:\n",
    "    \"\"\"Analyze items with permissive copyright but no accessible media.\"\"\"\n",
    "    analysis: AnalysisResults = {\n",
    "        \"total_items\": len(results),\n",
    "        \"permissive_copyright\": 0,\n",
    "        \"has_accessible_media\": 0,\n",
    "        \"permissive_with_media\": 0,\n",
    "        \"permissive_without_media\": 0,\n",
    "        \"gap_examples\": [],\n",
    "        \"permissive_licenses_found\": {},\n",
    "        \"media_availability_breakdown\": {\n",
    "            \"edm_is_shown_by\": 0,\n",
    "            \"edm_is_shown_at\": 0,\n",
    "            \"edm_preview\": 0,\n",
    "            \"web_resource\": 0,\n",
    "            \"no_media\": 0,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    print(f\"Analyzing {len(results)} items for copyright-media gap...\")\n",
    "\n",
    "    for _, item in enumerate(results):\n",
    "        # Check for permissive copyright\n",
    "        is_permissive, license_uri = is_permissive_copyright(item)\n",
    "        if is_permissive:\n",
    "            analysis[\"permissive_copyright\"] += 1\n",
    "\n",
    "            # Track which licenses we're finding\n",
    "            if license_uri in analysis[\"permissive_licenses_found\"]:\n",
    "                analysis[\"permissive_licenses_found\"][license_uri] += 1\n",
    "            else:\n",
    "                if license_uri is None:\n",
    "                    license_uri = \"Unknown\"\n",
    "                analysis[\"permissive_licenses_found\"][license_uri] = 1\n",
    "\n",
    "            # Check for accessible media\n",
    "            media_info = has_accessible_media(item)\n",
    "\n",
    "            if media_info[\"has_media\"]:\n",
    "                analysis[\"permissive_with_media\"] += 1\n",
    "                # Count media type availability\n",
    "                for media_type in media_info[\"media_types\"]:\n",
    "                    if media_type in analysis[\"media_availability_breakdown\"]:\n",
    "                        analysis[\"media_availability_breakdown\"][media_type] += 1\n",
    "            else:\n",
    "                analysis[\"permissive_without_media\"] += 1\n",
    "                analysis[\"media_availability_breakdown\"][\"no_media\"] += 1\n",
    "\n",
    "                # Collect examples for investigation\n",
    "                if len(analysis[\"gap_examples\"]) < 5:  # Keep first 5 examples\n",
    "                    gap_example: GapExample = {\n",
    "                        \"id\": item.id,\n",
    "                        \"title\": item.get_first_title(),\n",
    "                        \"provider\": (\n",
    "                            item.data_provider[0] if item.data_provider else \"Unknown\"\n",
    "                        ),\n",
    "                        \"license\": license_uri,\n",
    "                        \"link\": item.link,\n",
    "                    }\n",
    "                    analysis[\"gap_examples\"].append(gap_example)\n",
    "\n",
    "        # Overall media availability\n",
    "        media_info = has_accessible_media(item)\n",
    "        if media_info[\"has_media\"]:\n",
    "            analysis[\"has_accessible_media\"] += 1\n",
    "\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ubdm1emah2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_ex_4 = client.query().media_type(MediaType.IMAGE).country(\"Italy\").when(\"1960\")\n",
    "result_ex_4 = list(client.search_all(query_ex_4, max_records=25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m766kt5ym9h",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_results = analyze_permissive_no_media_gap(result_ex_4)\n",
    "\n",
    "print(f\"\\nOverview:\")\n",
    "print(f\"• Total items analyzed: {analysis_results['total_items']:,}\")\n",
    "print(\n",
    "    f\"• Items with accessible media: {analysis_results['has_accessible_media']:,} ({analysis_results['has_accessible_media']/analysis_results['total_items']*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"• Items with permissive copyright: {analysis_results['permissive_copyright']:,} ({analysis_results['permissive_copyright']/analysis_results['total_items']*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(f\"\\nThe Gap Analysis:\")\n",
    "print(\n",
    "    f\"• Permissive items WITH accessible media: {analysis_results['permissive_with_media']:,}\"\n",
    ")\n",
    "print(\n",
    "    f\"• Permissive items WITHOUT accessible media: {analysis_results['permissive_without_media']:,}\"\n",
    ")\n",
    "\n",
    "if analysis_results[\"permissive_copyright\"] > 0:\n",
    "    gap_percentage = (\n",
    "        analysis_results[\"permissive_without_media\"]\n",
    "        / analysis_results[\"permissive_copyright\"]\n",
    "    ) * 100\n",
    "    print(\n",
    "        f\"• Gap rate: {gap_percentage:.1f}% of permissive items lack accessible media\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nPermissive Licenses Found:\")\n",
    "for license_uri, count in sorted(\n",
    "    analysis_results[\"permissive_licenses_found\"].items(),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    "):\n",
    "    print(f\"• {count:,} items: {license_uri}\")\n",
    "\n",
    "print(f\"\\nMedia Availability Breakdown (for permissive items):\")\n",
    "for media_type, count in analysis_results[\"media_availability_breakdown\"].items():\n",
    "    if count > 0:\n",
    "        print(f\"• {media_type}: {count:,} items\")\n",
    "\n",
    "# Show examples of the gap\n",
    "if analysis_results[\"gap_examples\"]:\n",
    "    print(f\"\\nExamples of Permissive Copyright Items WITHOUT Accessible Media:\")\n",
    "    print(\"-\" * 60)\n",
    "    for i, example in enumerate(analysis_results[\"gap_examples\"], 1):\n",
    "        print(f\"{i}. {example['title'] or 'No title'}\")\n",
    "        print(f\"   Provider: {example['provider']}\")\n",
    "        print(f\"   License: {example['license']}\")\n",
    "        print(f\"   Link: {example['link']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9f836",
   "metadata": {},
   "source": [
    "### Analysis results\n",
    "\n",
    "Intrestingly enough we have not been able to find any cases where we have permissive licensing but possible restricted access to data. This might though just be a result of our limited sample size and what we are quering for. Feel free to try to create different querries that yield different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del result_ex_4\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f762",
   "metadata": {},
   "source": [
    "## Example 5: timestamp created vs updated, more complex analysis\n",
    "\n",
    "Same time/date created and updated may indicate that a collection data is outdated or underresearched. This can make it easier to identify topics that need revision or special attention.\n",
    "\n",
    "In this example we compare items kept at German institutions that have a Palestinian (dataset 1) and Israeli (dataset 2) origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb2dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for analysis\n",
    "def extract_timestamps(results):\n",
    "    \"\"\"Extract and parse timestamps from search results.\"\"\"\n",
    "    created_timestamps = []\n",
    "    updated_timestamps = []\n",
    "    missing_created = 0\n",
    "    missing_updated = 0\n",
    "    missing_both = 0\n",
    "\n",
    "    print(f\"Analyzing {len(results)} items for timestamp data...\")\n",
    "\n",
    "    for item in results:\n",
    "        created = item.timestamp_created\n",
    "        updated = item.timestamp_update\n",
    "\n",
    "        # Track missing data\n",
    "        if created is None and updated is None:\n",
    "            missing_both += 1\n",
    "        elif created is None:\n",
    "            missing_created += 1\n",
    "        elif updated is None:\n",
    "            missing_updated += 1\n",
    "\n",
    "        # Only add to plot data if both timestamps are available\n",
    "        if created is not None and updated is not None:\n",
    "            # Convert ISO timestamp strings to datetime\n",
    "            try:\n",
    "                created_dt = datetime.fromisoformat(created.replace(\"Z\", \"+00:00\"))\n",
    "                updated_dt = datetime.fromisoformat(updated.replace(\"Z\", \"+00:00\"))\n",
    "                created_timestamps.append(created_dt)\n",
    "                updated_timestamps.append(updated_dt)\n",
    "            except (ValueError, AttributeError) as e:\n",
    "                print(f\"Error parsing timestamps: {created}, {updated} - {e}\")\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"\\nTimestamp Statistics:\")\n",
    "    print(f\"Total items analyzed: {len(results)}\")\n",
    "    print(f\"Items with both timestamps: {len(created_timestamps)}\")\n",
    "    print(f\"Items missing creation timestamp only: {missing_created}\")\n",
    "    print(f\"Items missing update timestamp only: {missing_updated}\")\n",
    "    print(f\"Items missing both timestamps: {missing_both}\")\n",
    "    print(f\"Data availability: {len(created_timestamps)/len(results)*100:.1f}%\")\n",
    "\n",
    "    return created_timestamps, updated_timestamps\n",
    "\n",
    "\n",
    "def create_timestamp_plot(created_timestamps, updated_timestamps, location):\n",
    "    \"\"\"Create scatter plot for timestamp analysis.\"\"\"\n",
    "    if not created_timestamps:\n",
    "        print(f\"No data available for plotting {location}\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(created_timestamps, updated_timestamps, alpha=0.6, s=20)\n",
    "\n",
    "    # Add diagonal line to show where created = updated\n",
    "    min_date = min(min(created_timestamps), min(updated_timestamps))\n",
    "    max_date = max(max(created_timestamps), max(updated_timestamps))\n",
    "    plt.plot(\n",
    "        [min_date, max_date],\n",
    "        [min_date, max_date],\n",
    "        \"r--\",\n",
    "        alpha=0.7,\n",
    "        label=\"Created = Updated line\",\n",
    "    )\n",
    "\n",
    "    plt.xlabel(\"Timestamp Created\")\n",
    "    plt.ylabel(\"Timestamp Updated\")\n",
    "    plt.title(\n",
    "        f\"Creation vs Update Timestamps for {location} Items\\n({len(created_timestamps)} items with complete timestamp data)\"\n",
    "    )\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def perform_temporal_analysis(created_timestamps, updated_timestamps, location):\n",
    "    \"\"\"Perform comprehensive temporal analysis and print insights.\"\"\"\n",
    "    if not created_timestamps:\n",
    "        print(f\"No timestamp data available for {location} analysis\")\n",
    "        return\n",
    "\n",
    "    # Analyze items where created == updated (potential indicators of outdated/static collections)\n",
    "    same_timestamp_count = sum(\n",
    "        1 for c, u in zip(created_timestamps, updated_timestamps) if c == u\n",
    "    )\n",
    "    print(f\"\\n=== {location.upper()} ANALYSIS ===\")\n",
    "    print(\n",
    "        f\"Items with identical creation and update timestamps: {same_timestamp_count}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Percentage of items never updated: {same_timestamp_count/len(created_timestamps)*100:.1f}%\"\n",
    "    )\n",
    "\n",
    "    # Show time range of the data\n",
    "    earliest_created = min(created_timestamps)\n",
    "    latest_created = max(created_timestamps)\n",
    "    earliest_updated = min(updated_timestamps)\n",
    "    latest_updated = max(updated_timestamps)\n",
    "\n",
    "    print(f\"\\nTime Range Analysis:\")\n",
    "    print(f\"Earliest creation date: {earliest_created}\")\n",
    "    print(f\"Latest creation date: {latest_created}\")\n",
    "    print(f\"Earliest update date: {earliest_updated}\")\n",
    "    print(f\"Latest update date: {latest_updated}\")\n",
    "\n",
    "    # Calculate average update time (time between creation and update)\n",
    "    current_time = datetime.now(\n",
    "        created_timestamps[0].tzinfo\n",
    "    )  # Use same timezone as data\n",
    "    update_intervals = []\n",
    "    item_ages = []\n",
    "\n",
    "    for created, updated in zip(created_timestamps, updated_timestamps):\n",
    "        # Calculate time between creation and update\n",
    "        if created != updated:  # Only for items that were actually updated\n",
    "            update_interval = (updated - created).total_seconds() / (\n",
    "                24 * 3600\n",
    "            )  # Convert to days\n",
    "            update_intervals.append(update_interval)\n",
    "\n",
    "        # Calculate age of item (creation to now)\n",
    "        age = (current_time - created).total_seconds() / (24 * 3600)  # Convert to days\n",
    "        item_ages.append(age)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"TEMPORAL ANALYSIS - {location.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Average update time analysis\n",
    "    if update_intervals:\n",
    "        avg_update_time_days = np.mean(update_intervals)\n",
    "        median_update_time_days = np.median(update_intervals)\n",
    "        min_update_time_days = np.min(update_intervals)\n",
    "        max_update_time_days = np.max(update_intervals)\n",
    "        std_update_time_days = np.std(update_intervals)\n",
    "\n",
    "        print(f\"\\nUpdate Time Analysis (for {len(update_intervals)} updated items):\")\n",
    "        print(\n",
    "            f\"  Average time to update: {avg_update_time_days:.1f} days ({avg_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Median time to update:  {median_update_time_days:.1f} days ({median_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Fastest update:         {min_update_time_days:.1f} days\")\n",
    "        print(\n",
    "            f\"  Slowest update:         {max_update_time_days:.1f} days ({max_update_time_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Standard deviation:     {std_update_time_days:.1f} days\")\n",
    "    else:\n",
    "        print(f\"\\nUpdate Time Analysis:\")\n",
    "        print(f\"  No items were updated after creation (all have identical timestamps)\")\n",
    "\n",
    "    # Average age analysis\n",
    "    if item_ages:\n",
    "        avg_age_days = np.mean(item_ages)\n",
    "        median_age_days = np.median(item_ages)\n",
    "        min_age_days = np.min(item_ages)\n",
    "        max_age_days = np.max(item_ages)\n",
    "        std_age_days = np.std(item_ages)\n",
    "\n",
    "        print(f\"\\nItem Age Analysis (creation to present):\")\n",
    "        print(\n",
    "            f\"  Average age of items:   {avg_age_days:.1f} days ({avg_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Median age of items:    {median_age_days:.1f} days ({median_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(f\"  Newest item:            {min_age_days:.1f} days old\")\n",
    "        print(\n",
    "            f\"  Oldest item:            {max_age_days:.1f} days old ({max_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Standard deviation:     {std_age_days:.1f} days ({std_age_days/365.25:.1f} years)\"\n",
    "        )\n",
    "\n",
    "    # Summary insights\n",
    "    print(f\"\\n\" + \"=\" * 50)\n",
    "    print(f\"INSIGHTS - {location.upper()}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    never_updated_pct = same_timestamp_count / len(created_timestamps) * 100\n",
    "    print(f\"• {never_updated_pct:.1f}% of items have never been updated since creation\")\n",
    "\n",
    "    if update_intervals and len(update_intervals) > 0:\n",
    "        updated_pct = len(update_intervals) / len(created_timestamps) * 100\n",
    "        print(f\"• {updated_pct:.1f}% of items have been updated at least once\")\n",
    "        print(\n",
    "            f\"• Items that get updated wait an average of {avg_update_time_days/365.25:.1f} years before their first update\"\n",
    "        )\n",
    "\n",
    "    if item_ages:\n",
    "        print(\n",
    "            f\"• The average item in this collection is {avg_age_days/365.25:.1f} years old\"\n",
    "        )\n",
    "\n",
    "        # Age distribution insight\n",
    "        recent_items = sum(1 for age in item_ages if age <= 365)\n",
    "        old_items = sum(1 for age in item_ages if age >= 365 * 5)\n",
    "        recent_pct = recent_items / len(item_ages) * 100\n",
    "        old_pct = old_items / len(item_ages) * 100\n",
    "\n",
    "        print(f\"• {recent_pct:.1f}% of items were created within the last year\")\n",
    "        print(f\"• {old_pct:.1f}% of items are more than 5 years old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310cba1",
   "metadata": {},
   "source": [
    "### Fetch the data\n",
    "\n",
    "This can take some minutes if you are fetching more than 10000 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8aa87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch first dataset\n",
    "print(\"Fetching first dataset...\")\n",
    "query_1 = client.query().country(\"Germany\").where(\"Israel\")\n",
    "results_1 = list(client.search_all(query_1, max_records=10000))\n",
    "\n",
    "# Fetch second dataset\n",
    "print(\"Fetching second dataset...\")\n",
    "# We are using the German name for Palestine here since we are looking into items located in Germany.\n",
    "# For a more complete dataset we should probably do this query with more spellings.\n",
    "query_2 = client.query().country(\"Germany\").where(\"Palästina\")\n",
    "results_2 = list(client.search_all(query_2, max_records=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a8b8e",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vo8bkbbqvsr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis for Dataset 1\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 1 ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "created_1, updated_1 = extract_timestamps(results_1)\n",
    "create_timestamp_plot(created_1, updated_1, \"Dataset 1\")\n",
    "perform_temporal_analysis(created_1, updated_1, \"Dataset 1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET 2 ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run analysis for Dataset 2\n",
    "created_2, updated_2 = extract_timestamps(results_2)\n",
    "create_timestamp_plot(created_2, updated_2, \"Dataset 2\")\n",
    "perform_temporal_analysis(created_2, updated_2, \"Dataset 2\")\n",
    "\n",
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARATIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if created_1 and created_2:\n",
    "    never_updated_1 = sum(1 for c, u in zip(created_1, updated_1) if c == u)\n",
    "    never_updated_2 = sum(1 for c, u in zip(created_2, updated_2) if c == u)\n",
    "\n",
    "    never_pct_1 = never_updated_1 / len(created_1) * 100\n",
    "    never_pct_2 = never_updated_2 / len(created_2) * 100\n",
    "\n",
    "    print(f\"Dataset 1: {len(results_1)} items found, {len(created_1)} with timestamps\")\n",
    "    print(f\"Dataset 2: {len(results_2)} items found, {len(created_2)} with timestamps\")\n",
    "    print(f\"\\nNever updated comparison:\")\n",
    "    print(f\"• Dataset 1: {never_pct_1:.1f}% of items never updated\")\n",
    "    print(f\"• Dataset 2: {never_pct_2:.1f}% of items never updated\")\n",
    "\n",
    "    if never_pct_1 > never_pct_2:\n",
    "        diff = never_pct_1 - never_pct_2\n",
    "        print(\n",
    "            f\"• Dataset 1 has {diff:.1f} percentage points more items that were never updated\"\n",
    "        )\n",
    "    elif never_pct_2 > never_pct_1:\n",
    "        diff = never_pct_2 - never_pct_1\n",
    "        print(\n",
    "            f\"• Dataset 2 has {diff:.1f} percentage points more items that were never updated\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"• Both datasets have similar rates of items never updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b4732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del results_1\n",
    "del results_2\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
